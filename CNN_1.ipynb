{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j1212392/AI-1/blob/110-2/CNN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JaBjQKoedCrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "KSX05gFRdC0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -U\n",
        "!pip install nlp -U\n",
        "!pip install torch -U"
      ],
      "metadata": {
        "id": "GFD804_rdIYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
        "from nlp import load_dataset, Dataset\n",
        "import nlp\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "lv4KCq70dXRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "dataset = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "eV0oYFaedXp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(10)"
      ],
      "metadata": {
        "id": "E90ikPh0dJLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('hfl/chinese-bert-wwm-ext')"
      ],
      "metadata": {
        "id": "OQMiSvnedbEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ykrsokxYdcVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 5\n",
        "MAX_LEN = 512\n",
        "EPOCHS=5\n",
        "BATCH_SIZE = 8"
      ],
      "metadata": {
        "id": "OpCOiOyndfgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('hfl/chinese-bert-wwm-ext')"
      ],
      "metadata": {
        "id": "85TL6Fb-ddS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_ds = dataset.shuffle(RANDOM_SEED)\n",
        "split_ds = shuffled_ds.train_test_split(test_size=0.2)\n",
        "train_dataset = split_ds['train']\n",
        "test_val_dataset = split_ds['test']\n",
        "split_tv = test_val_dataset.train_test_split(test_size=0.5)\n",
        "test_dataset = split_tv['train']\n",
        "val_dataset = split_tv['test']"
      ],
      "metadata": {
        "id": "ACL0kU62diA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[2]"
      ],
      "metadata": {
        "id": "PZacmDJndikt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"體重跟體脂都有在持續緩慢的下降中\")"
      ],
      "metadata": {
        "id": "khbjVuRjdkZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "  return tokenizer(batch['Description'], max_length=MAX_LEN, padding=True, truncation=True)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
        "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
        "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
        "\n",
        "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
      ],
      "metadata": {
        "id": "_dm63fsOdl2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    # evaluate_during_training=True,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "Gr13iksVdnb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "EhcuC8m-domH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save_pretrained(\"bert_food_ad\")"
      ],
      "metadata": {
        "id": "BbvnTp7Idp2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir='./logs' # --host 192.168.1.113\n",
        "#from tensorboard import notebook\n",
        "#notebook.list()\n",
        "#notebook.display(port=6006, height=1000)"
      ],
      "metadata": {
        "id": "dvHQGQ6pdrXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import BertConfig\n",
        "\n",
        "test_config = BertConfig.from_json_file('./bert_food_ad/config.json')\n",
        "test_model = BertForSequenceClassification.from_pretrained('./bert_food_ad/pytorch_model.bin', config=test_config)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "5CSwJzwpdtKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(df.iloc[220]['label'], df.iloc[220]['Description'])\n",
        "print(df.iloc[578]['label'], df.iloc[578]['Description'])"
      ],
      "metadata": {
        "id": "nTaA5YX2dvoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_text = '增強免疫力...護眼明目...抗氧化...保肝...皮膚：...保護皮膚，減緩皮屑問題，降低敏感...毛髮……促進毛髮亮麗...消化系統」「強化免疫系統，有助調整過敏體質...清除自由基延緩老化...調整腸胃道機能...調理皮膚問題...諾麗果含有許多強大的抗氧化劑」「阻止或減少病痛的發生，並迅速修補已受傷的細胞，將代謝殘渣或毒素排出體外」「它能維護身體細胞組織正常運作，阻止或減少病痛的發生，並迅速修補已受傷的細胞，將代謝殘渣或毒素排出體外，使身體恢復正常'\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "encoded_review = tokenizer.encode_plus(\n",
        "    test_text,\n",
        "    max_length=MAX_LEN,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=False,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        "    truncation=True\n",
        ")"
      ],
      "metadata": {
        "id": "4IGoI-UgdwPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_review"
      ],
      "metadata": {
        "id": "DTrrrn92dx6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_model = test_model.to('cuda')\n",
        "\n",
        "input_ids = encoded_review['input_ids'].to('cuda')\n",
        "attention_mask = encoded_review['attention_mask'].to('cuda')\n",
        "output = test_model(input_ids, attention_mask)\n",
        "result = torch.argmax(output[0][0])\n",
        "\n",
        "classnames = ['Passed', 'Failed']\n",
        "\n",
        "print(f'Review text: {test_text}')\n",
        "print(f'Result  : {classnames[result]}')"
      ],
      "metadata": {
        "id": "Nxjr2pTHdz8t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}